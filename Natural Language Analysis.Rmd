---
title: "Natural Language Analysis"
author: '[Ryo, Eng Lian Hu](http://englianhu.wordpress.com) <img src=''figure/TonyStark.png''
  width=''24''> TonyStark®'
date: "9/22/2015"
output:
  html_document:
    fig_height: 3
    fig_width: 5
    highlight: haddock
    theme: cerulean
    toc: yes
  pdf_document:
    fig_height: 3
    fig_width: 5
    highlight: haddock
    toc: yes
---

This is an natural language analysis on the matching soccer teams' name when I am doing research on [Betting Strategy and Model Validation](https://github.com/Scibrokes/Betting-Strategy-and-Model-Validation/blob/master/Betting%20Strategy%20and%20Model%20Validation.Rmd). Where the subject/topic is that the last course [Data Science Capstone](https://www.coursera.org/course/dsscapstone) on Coursera (JHU Johns Hopkins University) which I have failed few times and will retake on this coming October-2015 (Next month).

Note that the `echo = FALSE` and `include=FALSE` parameters were added to the code chunks below to prevent printing of the R code that generated the plots/tables. However you can feel free to see the source code via [Natural Language Analysis.Rmd](https://github.com/Scibrokes/Betting-Strategy-and-Model-Validation/Natural Language Analysis.Rmd).



## 1. Setup Options, Loading Required Libraries and Preparing Environment

Setup `knitr` options and loading the required libraries.

```{r load-packages, include=FALSE}
## You can write as ```{r load-packages, include=FALSE} if you want to hide the particular chunk
## Setting to omitt all warnings
options(warn=-1)

## Loading the packages
if(!'devtools' %in% installed.packages()){
  install.packages('devtools')}
if(!'BBmisc' %in% installed.packages()){
  install.packages('BBmisc')}

suppressPackageStartupMessages(library('BBmisc'))
pkgs <- c('devtools','stringr','stringi','reshape','reshape2','data.table','sparkline','DT','plyr','dplyr','magrittr','foreach','doParallel','rmarkdown','tidyr','gtable','grid','gridExtra','pander','stringdist','knitr','rmarkdown','lubridate','d3Network','networkD3')
suppressAll(lib(pkgs)); rm(pkgs)
```

Creating a parallel computing Cluster and support functions.

```{r setting, include=FALSE}
## Preparing the parallel cluster using the cores
doParallel::registerDoParallel(cores = 16)
#' @BiocParallel::register(MulticoreParam(workers=2))

## Make pretty table
## http://kbroman.org/knitr_knutshell/pages/figs_tables.html
## knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA, fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6, fig.align="center")

## Table width setting
panderOptions('table.split.table', Inf)
```



## 2. Read and Process the Dataset

Read the dataset of World Wide soccer matches from year 2011 until 2015 from a British betting consultancy named firm A.

```{r read-datasetA, echo=FALSE, results='asis'}
## Read the datasets
## Refer to **Testing efficiency of coding.Rmd** at chunk `get-data-summary-table-2.1`
source(paste0(getwd(),'/function/readfirmDatasets.R'))
years <- seq(2011,2015)
mbase <- readfirmDatasets(years=years)
dateID <- sort(unique(mbase$datasets$Date))
#'@ pander(head(mbase$datasets)) ## exactly same layout with kable(x)

## example of the dataset in the research paper
##   due to the data heavy and overload ,max 5mb while generated 79mb html file, here I just simply subset the head section
mbase$datasets %>% head %>% kable
```

*table 2.1* `r paste(unlist(strsplit(as.character(dim(mbase$datasets)),' ')), collapse=' x ')`

Due to the dataset very big `r paste(unlist(strsplit(as.character(dim(mbase$datasets)),' ')), collapse=' x ')` caused the webpage keep loading and unable open. Here I just only subset few rows from the data frame.

Read the dataset of World Wide soccer matches scrapped from year 2011 until 2015 from [spbo livescore website](http://www.spbo.com/eend0.htm).

```{r read-datasetB, echo=FALSE, results='asis'}
## Load the scraped spbo livescore datasets.
source(paste0(getwd(),'/function/readSPBO2.R'))
spboData <- readSPBO2(dateID=dateID, parallel=FALSE)
## example of the scrapped livescore dataset in the research paper
##   due to the data heavy and overload ,max 5mb while generated 79mb html file, here I just simply subset the head section
spboData %>% head %>% kable 
```

*table 2.2* `r paste(unlist(strsplit(as.character(dim(spboData)),' ')), collapse=' x ')`

Due to the dataset very big `r paste(unlist(strsplit(as.character(dim(spboData)),' ')), collapse=' x ')` caused the webpage keep loading and unable open. Here I just only subset few rows from the data frame.



## 3. Matching the team names


### 3.1 Matching Duplicated Teams' Name

  In order to matching a string. Firstly we can apply `match()` or `%in%` to matching the teams' name. Although, the capital letter different is not duplicated string in R programming while I apply the `tolower()` to match the teams' name since it is consider exactly matching teams' name in our real life.

```{r matching-01, echo=FALSE, results='asis'}
## Get and filter the teams' name
## Filter and drop the first-half, corners and other games
teamID <- sort(unique(c(as.character(mbase$datasets$Home), as.character(mbase$datasets$Away))))
teamID <- teamID[!teamID %in% mbase$others]

spboTeam <- sort(c(as.vector(spboData$Home), as.vector(spboData$Away)))
spboTeamID <- sort(unique(spboTeam))

df1 <- data.frame(team=teamID[tolower(teamID) %in% tolower(spboTeamID)], spbo=spboTeamID[tolower(spboTeamID) %in% tolower(teamID)]) %>% tbl_df %>% mutate(team=as.character(team),spbo=as.character(spbo),pass=ifelse(team==spbo,'Duplicated','Capital Letters')) %>% arrange(pass)
row.names(df1) <- NULL
rbind(df1 %>% filter(pass=='Duplicated') %>% head(3),df1 %>% filter(pass=='Capital Letters') %>% head(3)) %>% kable
```

*table 3.1.1* `r paste(unlist(strsplit(as.character(dim(df1)),' ')), collapse=' x ')`


### 3.2 Apply stringdist()

  There has a concern which is noramlly second teams' name must be exactly same with first team but only add II, reserved etc to the first team name, for example : *Mainz 05* is first team but not fifth reserved team. More soccer matches data scrapped will be more accurate, for example if we only scrapped one day data, how can we matching the first team if let say only Chelsea reserved team play on that particular date.
  
  However there has another concern which is first team *TSV 1860 Munchen* but second/U19 team termed as *1860 Munchen II*, *1860 Munchen U19* etc. The *Lincoln* team name supposed to be matched with *Lincoln City* but not *Lincoln United* while *Lincoln Citx* will be most approximately matching to *Lincoln Xxitxx* compare to *Lincoln*.
  
  Besides, if I set the priority of matching the kick-off date and later team names, it will be a concern of possibilities of postponed staked matches (postponed after firm A placed bets, sometimes firm A will placed bets on Early market or the kick-off date accidentially changed/postponed before kick-off due to snowing/downpour/etc).
  
  I load the [`stringdist`](https://cran.r-project.org/web/packages/stringdist/index.html) package to apply the algorithmic matching the team names.
  
  * 01. osa - Optimal string aligment, (restricted Damerau-Levenshtein distance).
  * 02. lv - Levenshtein distance (as in R’s native adist).
  * 03. dl - Full Damerau-Levenshtein distance.
  * 04. hamming - Hamming distance (a and b must have same nr of characters).
  * 05. lcs - Longest common substring distance.
  * 06. qgram - q-gram distance.
  * 07. cosine - cosine distance between q-gram profiles.
  * 08. jaccard - Jaccard distance between q-gram profiles.
  * 09. jw - Jaro, or Jaro-Winker distance.
  * 10. soundex - Distance based on soundex encoding (see below).
  
  Lets take an example below.
  
```{r matching-02A, echo=FALSE, results='asis'}
## Apply stringdist() to match the most approximate matching team names
method=c('osa','lv','dl','hamming','lcs','qgram','cosine','jaccard','jw','soundex')
#'@ levDist=0.1 # The default MaxDist inside stringdist() is 0.1.

strList <- function(team_a, team_b, method, levDist=NULL){
  unlist(llply(as.list(method), function(x){
    if(is.null(levDist)){
      levDist=min(stringdist(team_a, team_b, method=x))
    }else if(is.numeric(levDist)){
      levDist=levDist
    }else{
      stop('Please enter a numeric or just keep default NULL value on levDist!')
    }
    if(!method %in% c('osa','lv','dl','hamming','lcs','qgram','cosine','jaccard','jw','soundex')){
      stop('Please enter value within "osa","lv","dl","hamming","lcs","qgram","cosine","jaccard","jw","soundex")')
    }
    team_b[amatch(team_a, team_b, method=x, maxDist=levDist)]
  },.parallel=FALSE))
}

## Check how many teams' name includes string 'Lincoln'.
teamID[grep('Lincoln',teamID)]

lst <- list(uniqueID_0.1=strList('Lincoln',spboTeamID,method=method,levDist=0.1),
            allElems_0.1=strList('Lincoln',spboTeam,method=method,levDist=0.1),
            uniqueID_0.5=strList('Lincoln',spboTeamID,method=method,levDist=0.5),
            allElems_0.5=strList('Lincoln',spboTeam,method=method,levDist=0.5),
            uniqueID_1.0=strList('Lincoln',spboTeamID,method=method,levDist=1.0),
            allElems_1.0=strList('Lincoln',spboTeam,method=method,levDist=1.0),
            uniqueID_2.0=strList('Lincoln',spboTeamID,method=method,levDist=2.0),
            allElems_2.0=strList('Lincoln',spboTeam,method=method,levDist=2.0),
            uniqueID_Inf=strList('Lincoln',spboTeamID,method=method,levDist=Inf),
            allElems_Inf=strList('Lincoln',spboTeam,method=method,levDist=Inf))
len <- sapply(lst,length)
n <- max(len)
len <- n-len

df2A <- mapply(function(x,y) c(x, rep(NA, y)), lst, len) %>% data.frame %>% mutate(Matching1='Lincoln',Matching2='Lincoln City',method=method) %>% select(Matching1,method,uniqueID_0.1,allElems_0.1,uniqueID_0.5,allElems_0.5,uniqueID_1.0,allElems_1.0,uniqueID_2.0,allElems_2.0,uniqueID_Inf,allElems_Inf) %>% tbl_df
rm(lst,len,n)
kable(df2A)
```

*table 3.2.1* `r paste(unlist(strsplit(as.character(dim(df2A)),' ')), collapse=' x ')`

  I simply matching the key words `Lincoln` in Home and Away teams' name data which get from firm A.

```{r matching-02B, echo=FALSE, results='asis'}
## We get 'Lincoln City' from teamID[grep('Lincoln',teamID)]
lst <- list(uniqueID_0.1=strList('Lincoln City',spboTeamID,method=method,levDist=0.1),
            allElems_0.1=strList('Lincoln City',spboTeam,method=method,levDist=0.1),
            uniqueID_0.5=strList('Lincoln City',spboTeamID,method=method,levDist=0.5),
            allElems_0.5=strList('Lincoln City',spboTeam,method=method,levDist=0.5),
            uniqueID_1.0=strList('Lincoln City',spboTeamID,method=method,levDist=1.0),
            allElems_1.0=strList('Lincoln City',spboTeam,method=method,levDist=1.0),
            uniqueID_2.0=strList('Lincoln City',spboTeamID,method=method,levDist=2.0),
            allElems_2.0=strList('Lincoln City',spboTeam,method=method,levDist=2.0),
            uniqueID_Inf=strList('Lincoln City',spboTeamID,method=method,levDist=Inf),
            allElems_Inf=strList('Lincoln City',spboTeam,method=method,levDist=Inf))
len <- sapply(lst,length)
n <- max(len)
len <- n-len

df2B <- mapply(function(x,y) c(x, rep(NA, y)), lst, len) %>% data.frame %>% mutate(Matching1='Lincoln',Matching2='Lincoln City',method=method) %>% select(Matching1,method,uniqueID_0.1,allElems_0.1,uniqueID_0.5,allElems_0.5,uniqueID_1.0,allElems_1.0,uniqueID_2.0,allElems_2.0,uniqueID_Inf,allElems_Inf) %>% tbl_df
rm(lst,len,n)
kable(df2B)
```

*table 3.2.2* `r paste(unlist(strsplit(as.character(dim(df2B)),' ')), collapse=' x ')`

  From the two tables stated above, I apply stringdist by set the MaxDist to be default value `0.1`,`0.5`,`1.0`,`2.0` and also `Inf` and select all methods avaiable (10 methods stated above in section 3 before the run coding). Well, I dont pretend to know how does the algorimthic of `stringdist()` matching the string. Therefore I try both unique teams' name and also all elements (without filter to be unique).


### 3.3 Apply agrep()

  Secondly, there is an article from [Merging Data Sets Based on Partially Matched Data Elements](http://www.r-bloggers.com/merging-data-sets-based-on-partially-matched-data-elements/) which apply while loop.

```{r matching-03, echo=FALSE, results='asis'}
## Filter spboTeamID wihthout other games and 1st Half team names
## Apply agrep() to match the most approximate matching team names
## http://stackoverflow.com/questions/21103410/irregular-list-of-lists-to-dataframe
lst <- list(team1=sort(unique(c(teamID[agrep('Lincoln',teamID)]))),spbo1=sort(unique(c(spboTeamID[agrep('Lincoln',spboTeamID)]))), team2=sort(unique(c(teamID[agrep('Lincoln City',teamID)]))),spbo2=sort(unique(c(spboTeamID[agrep('Lincoln City',spboTeamID)]))))
len <- sapply(lst,length)
n <- max(len)
len <- n-len

df3 <- mapply(function(x,y) c(x, rep(NA, y)), lst, len) %>% data.frame %>% mutate(Matching1='Lincoln',Matching2='Lincoln City') %>% select(Matching1,team1,spbo1,Matching2,team2,spbo2) %>% tbl_df
rm(lst,len,n,len)
kable(df3)
```

*table 3.3.1* `r paste(unlist(strsplit(as.character(dim(df3)),' ')), collapse=' x ')`


### 3.4 Apply partialMatch()

  There is a partialMatch() function from [How can I match fuzzy match strings from two datasets?](http://stackoverflow.com/questions/26405895/how-can-i-match-fuzzy-match-strings-from-two-datasets) which apply Expectation Maximization theory by using while loop on `agrep()`.

```{r matching-04, echo=FALSE, results='asis'}
## Load the partialMatch() function
source(paste0(getwd(),'/function/partialMatch.R'))
pmID <- iconv(teamID)
df4 <- partialMatch(pmID, spboTeamID) %>% rename(team=raw.x,spbo=raw.y) %>% tbl_df
row.names(df4) <- NULL
rm(pmID)
#'@ rbind(df4 %>% filter(pass=='Duplicate') %>% head(3),df4 %>% filter(pass=='Partial') %>% head(3)) %>% kable
df4 %>% datatable
```

*table 3.4.1* `r paste(unlist(strsplit(as.character(dim(df4)),' ')), collapse=' x ')`

  Above table simply display 3 matched teams' name for each duplicate and partial matching.

```{r matching-04B, echo=FALSE, results='asis'}
df4 %>% filter((team %in% team[grep('Women|U[0-9]{2}',team)])|(spbo %in% spbo[grep('Women|U[0-9]{2}',spbo)]), pass=='Partial') %>% head %>% kable
```

*table 3.4.2* `r paste(unlist(strsplit(as.character(dim(df4)),' ')), collapse=' x ')`

  From the table above we all know that even though we purely apply EM theory on the raw data, the team `AaB Aalborg` from firm A will match with `AaB Aalborg U17` from livescore website and `Airdrie United` match to `Airdrie United Women` while there are totally different team and will lead reasearcher calculate a wrong predictive figures for investment.

  In order to maximized the soccer matches (observations) available for the research, here I seperates few steps to matching the teams' name by using `split()` and cross-matching each others (`spbo[grep(team,spbo)]` and `spbo[grep(spbo,team)]`) to seperately rearrange the data prior to start the algorithmic matching function in **section 4 Reprocess the Data**.



## 4. Reprocess the Data


### 4.1 Dicission Tree

```{r matching-05A, echo=FALSE, results='asis'}
source(paste0(getwd(),'/function/makeList.R'))
## Since the elements are not much enough but list quit a number, just set parallel=FALSE will be faster few minutes.
dfm <- makeList(mbase, spboData, levDist=0.1, parallel=FALSE)
```

I would like to plot a hierarchical chart for spliting teams' name for `agrep`. However due to `rpart` and `randomForest` packages required numeric data while diagram doesn't special. Here I plot two dynamic graphs.

```{r decission-tree-A, echo=FALSE, results='asis'}
## Refered `rpart` and `randomForest` to plot the decision tree, showing how to match the teams' name but required numeric data and also static graph.
## Here I using `d3Network`
d3data <- dfm$data %>% data.frame
d3data$X1 <- ifelse(is.na(d3data$X1),'NA',d3data$X1)
#'@ d3data %>% d3SimpleNetwork(.,width='automatic', height=400) ## https://github.com/christophergandrud/d3Network/issues/31
#'@ d3data %>% as.list %>% d3Tree ## https://github.com/christophergandrud/d3Network/issues/31
d3data[1:2] %>% simpleNetwork(.,width=400, height=400)
```

Since the `simpleNetwork` function only apply to 2 columns dataset, here I split to be 2 graphs.

```{r decission-tree-B, echo=FALSE, results='asis'}
d3data[3:4] %>% simpleNetwork(.,width='auto', height=800)
```


### 4.2 Filtering and Reprocess the Data

Here I tried to `split` teams' name into list to matching.

```{r matching-05B, echo=FALSE, results='asis'}
datatable(dfm$data)
```


### 4.3 StringDist

```{r stringdist, echo=FALSE, results='asis'}
source('.R')

```



## 5. Appendices

  
### 5.1 Documenting File Creation 

It's useful to record some information about how your file was created.

  * File creation date: `r Sys.Date()`
  * `r R.version.string`
  * R version (short form): `r getRversion()`
  * `rmarkdown` package version: `r packageVersion('rmarkdown')`
  * File package version: 1.0
  * Additional session information
  
```{r echo=FALSE, results='asis'}
lubridate::now()
devtools::session_info()$platform
Sys.info()
```



### 5.2 References

  * [Merging Data Sets Based on Partially Matched Data Elements](http://www.r-bloggers.com/merging-data-sets-based-on-partially-matched-data-elements/)
  * [How can I match fuzzy match strings from two datasets?](http://stackoverflow.com/questions/26405895/how-can-i-match-fuzzy-match-strings-from-two-datasets)
  * [d3Network](http://christophergandrud.github.io/d3Network/)


